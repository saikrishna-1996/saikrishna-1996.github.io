@article{kharyal2025do,
  title={Do as you teach: a multi-teacher approach to self-play in deep reinforcement learning},
  author={Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay Kumar and others},
  journal={Neural Computing and Applications},
  volume={37},
  pages={5945--5956},
  year={2025},
  doi={10.1007/s00521-024-10829-4},
  url={https://doi.org/10.1007/s00521-024-10829-4}
}

@incollection{icml2020_6186,
 abstract = {Over the last decade, there has been significant progress in the field of machine learning-based de novo drug discovery, particularly in generative modeling of chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure the synthetic accessibility nor provide the synthetic routes of the proposed small molecules which limits their applicability. In this work, we propose a novel reinforcement learning (RL) setup for drug discovery that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo compound design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. Our end-to-end approach achieves state-of-the-art performance when compared against other generative approaches for drug discovery. Moreover, we leverage our approach in a proof-of-concept that mimics the drug discovery process by generating novel HIV drug candidates. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.},
 author = {Gottipati, Sai Krishna and Sattarov, Boris and Niu, Sufeng and Wei, Haoran and Pathak, Yashaswi and Liu, Shengchao and Blackburn, Simon and Thomas, Karam and Coley, Connor and Tang, Jian and Chandar, Sarath and Bengio, Yoshua},
 booktitle = {Proceedings of Machine Learning and Systems 2020},
 pages = {10776--10787},
 title = {Learning to Navigate in Synthetically Accessible Chemical Space Using Reinforcement Learning},
 year = {2020}
}

@ARTICLE{8936918,
author={T. {Ort} and K. {Murthy} and R. {Banerjee} and S. K. {Gottipati} and D. {Bhatt}
and I. {Gilitschenski} and L. {Paull} and D. {Rus}},
journal={IEEE Robotics and Automation Letters},
title={MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map},
year={2020},  volume={5},
number={2},  pages={556-563},}


@article{8784238,
author={S. K. {Gottipati} and K. {Seo} and D. {Bhatt} and V. {Mai} and K. {Murthy} and L. {Paull}},
journal={IEEE Robotics and Automation Letters},
title={Deep Active Localization},
year={2019},
volume={4},  number={4},
pages={4394-4401},
}

@INPROCEEDINGS{7989089,
  author={J. K. {Murthy} and G. V. S. {Krishna} and F. {Chhaya} and K. M. {Krishna}},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title={Reconstructing vehicles from a single image: Shape priors for road scene understanding},
  year={2017},
  volume={},
  number={},
  pages={724-731},}

@article{sai2018deep,
  title={Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting},
  author={Sai Krishna G, V and Goyette, Kyle and Chamseddine, Ahmad and Considine, Breandan},
  journal={preprint arXiv:1806.00683},
  year={2018}
}

@article{DBLP:journals/corr/abs-2010-03744,
  author       = {Sai Krishna Gottipati and
                  Yashaswi Pathak and
                  Rohan Nuttall and
                  Sahir and
                  Raviteja Chunduru and
                  Ahmed Touati and
                  Sriram Ganapathi Subramanian and
                  Matthew E. Taylor and
                  Sarath Chandar},
  title        = {Maximum Reward Formulation In Reinforcement Learning},
  journal      = {NeurIPS 2020 Deep RL Workshop},
  volume       = {abs/2010.03744},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.03744},
  eprinttype    = {arXiv},
  eprint       = {2010.03744},
  timestamp    = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-03744.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2106-11345,
  author       = {A. I. Redefined and
                  Sai Krishna Gottipati and
                  Sagar Kurandwad and
                  Clod{\'{e}}ric Mars and
                  Gregory Szriftgiser and
                  Fran{\c{c}}ois Chabot},
  title        = {Cogment: Open Source Framework For Distributed Multi-actor Training,
                  Deployment {\&} Operations},
  journal      = {AAMAS 2022 workshop on Multi-Agent Reinforcement Learning},
  volume       = {abs/2106.11345},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.11345},
  eprinttype    = {arXiv},
  eprint       = {2106.11345},
  timestamp    = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-11345.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Gottipati_Pathak_Sattarov_Sahir_Nuttall_Amini_Taylor_Chandar_2021, title={Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16087}, DOI={10.1609/aaai.v35i1.16087}, abstractNote={Reinforcement learning (RL) has made significant progress in both abstract and real-world domains, but the majority of state-of-the-art algorithms deal only with monotonic actions. However, some applications require agents to reason over different types of actions. Our application simulates reaction-based molecule generation, used as part of the drug discovery pipeline, and includes both uni-molecular and bi-molecular reactions. This paper introduces a novel framework, towered actor critic (TAC), to handle multiple action types. The TAC framework is general in that it is designed to be combined with any existing RL algorithms for continuous action space. We combine it with TD3 to empirically obtain significantly better results than existing methods in the drug discovery setting. TAC is also applied to RL benchmarks in OpenAI Gym and results show that our framework can improve, or at least does not hurt, performance relative to standard TD3.}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gottipati, Sai Krishna and Pathak, Yashaswi and Sattarov, Boris and Sahir, and Nuttall, Rohan and Amini, Mohammad and Taylor, Matthew E. and Chandar, Sarath}, year={2021}, month={May}, pages={142-150} }

@article{islam2025human,
  title={Human-AI collaboration in real-world complex environment with reinforcement learning},
  author={Islam, Md Saiful and Das, Srijita and Gottipati, Sai Krishna and others},
  journal={Neural Computing and Applications},
  year={2025},
  doi={10.1007/s00521-025-11288-1},
  url={https://doi.org/10.1007/s00521-025-11288-1}
}

@misc{korablyov2024generativeactivelearningsearch,
      title={Generative Active Learning for the Search of Small-molecule Protein Binders}, 
      author={Maksym Korablyov and Cheng-Hao Liu and Moksh Jain and Almer M. van der Sloot and Eric Jolicoeur and Edward Ruediger and Andrei Cristian Nica and Emmanuel Bengio and Kostiantyn Lapchevskyi and Daniel St-Cyr and Doris Alexandra Schuetz and Victor Ion Butoi and Jarrid Rector-Brooks and Simon Blackburn and Leo Feng and Hadi Nekoei and SaiKrishna Gottipati and Priyesh Vijayan and Prateek Gupta and Ladislav Rampášek and Sasikanth Avancha and Pierre-Luc Bacon and William L. Hamilton and Brooks Paige and Sanchit Misra and Stanislaw Kamil Jastrzebski and Bharat Kaul and Doina Precup and José Miguel Hernández-Lobato and Marwin Segler and Michael Bronstein and Anne Marinier and Mike Tyers and Yoshua Bengio},
      year={2024},
      eprint={2405.01616},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM},
      url={https://arxiv.org/abs/2405.01616}, 
}

@inproceedings{10.5555/3545946.3599174,
author = {Gottipati, Sai Krishna and Nguyen, Luong-Ha and Mars, Clod\'{e}ric and Taylor, Matthew E.},
title = {Hiking up that HILL with Cogment-Verse: Train \& Operate Multi-agent Systems Learning from Humans},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As more AI systems are deployed, humans are increasingly required to interact with them in multiple settings. However, such AI systems seldom learn from these interactions with humans, which provides an important opportunity to improve from human expertise and context awareness. Several recent results in the fields of reinforcement learning (RL) and human-in-the-loop learning (HILL) show that AI agents can perform better when humans are involved in their training process. Humans can provide rewards to the agent, demonstrate tasks, design curricula, or act directly in the environment, but these potential performance improvements also come with architectural, functional design, and engineering complexities. This paper discusses Cogment, a unifying open-source framework that introduces a formalism to support a variety of human(s)-agent(s) collaboration topologies and training approaches. Cogment addresses the complexity of training with humans within a production-ready platform. On top of Cogment, we introduce Cogment Verse a research platform dedicated to the research community to facilitate the implementation of HILL and Multi-Agent RL experiments. With these platforms, our end goal is to enable the generalization of intelligence ecosystems where AI agents and humans learn from each other and collaborate to address increasingly complex or sensitive use cases. The video demonstration is available at https://youtu.be/v-K0DqIL9K0},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {3065–3067},
numpages = {3},
keywords = {human-in-the-loop, multi agent rl, open source, real world applications},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{mars2023human,
  title={Human-AI Interactions: A Workshop on Multi-Agent Systems},
  author={Mars, Clod{\'e}ric and Gottipati, Sai Krishna and others},
  booktitle={Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year={2023},
  note={Workshop on Human-AI Interactions},
  url={https://www.cloderic.com/content/2023-05-29-ala-aamas-human-ai-interactions/human-ai-interactions.pdf}
}

@inproceedings{10.5555/3635637.3663151,
author = {Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay and Das, Srijita and Taylor, Matthew E.},
title = {GLIDE-RL: Grounded Language Instruction through DEmonstration in RL},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A critical capability in the development of complex human-AI collaborative systems is the ability of AI agents to understand the natural language and perform tasks accordingly.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2333–2335},
numpages = {3},
keywords = {curriculum learning, grounded language, reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@misc{moujtahid2023humanmachineteaminguavsexperimentation,
      title={Human-Machine Teaming for UAVs: An Experimentation Platform}, 
      author={Laila El Moujtahid and Sai Krishna Gottipati and Clodéric Mars and Matthew E. Taylor},
      year={2023},
      eprint={2312.11718},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.11718}, 
}

@misc{arabneydi2025systematicapproachdesignrealworld,
      title={A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs}, 
      author={Jalal Arabneydi and Saiful Islam and Srijita Das and Sai Krishna Gottipati and William Duguay and Cloderic Mars and Matthew E. Taylor and Matthew Guzdial and Antoine Fagette and Younes Zerouali},
      year={2025},
      eprint={2504.17006},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.17006}, 
}


@misc{bhati2023curriculumlearningcooperationmultiagent,
      title={Curriculum Learning for Cooperation in Multi-Agent Reinforcement Learning}, 
      author={Rupali Bhati and Sai Krishna Gottipati and Clodéric Mars and Matthew E. Taylor},
      year={2023},
      eprint={2312.11768},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.11768}, 
}

@inproceedings{aasg2024_paper_5,
  title={Autonomous Agents for Social Good: Workshop Paper},
  author={Gottipati, Sai Krishna and others},
  booktitle={Proceedings of the Autonomous Agents for Social Good Workshop at AAMAS 2024},
  year={2024},
  location={Auckland, New Zealand},
  note={Workshop on Autonomous Agents for Social Good (AASG)},
  url={https://panosd.eu/aasg2024/papers/AASG2024_paper_5.pdf}
}

