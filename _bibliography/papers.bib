@article{kharyal2025do,
  title={Do as you teach: a multi-teacher approach to self-play in deep reinforcement learning},
  author={Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay Kumar and others},
  journal={Neural Computing and Applications},
  volume={37},
  pages={5945--5956},
  year={2025},
  doi={10.1007/s00521-024-10829-4},
  url={https://doi.org/10.1007/s00521-024-10829-4}
}

@incollection{icml2020_6186,
 abstract = {Over the last decade, there has been significant progress in the field of machine learning-based de novo drug discovery, particularly in generative modeling of chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure the synthetic accessibility nor provide the synthetic routes of the proposed small molecules which limits their applicability. In this work, we propose a novel reinforcement learning (RL) setup for drug discovery that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo compound design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. Our end-to-end approach achieves state-of-the-art performance when compared against other generative approaches for drug discovery. Moreover, we leverage our approach in a proof-of-concept that mimics the drug discovery process by generating novel HIV drug candidates. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.},
 author = {Gottipati, Sai Krishna and Sattarov, Boris and Niu, Sufeng and Wei, Haoran and Pathak, Yashaswi and Liu, Shengchao and Blackburn, Simon and Thomas, Karam and Coley, Connor and Tang, Jian and Chandar, Sarath and Bengio, Yoshua},
 booktitle = {Proceedings of Machine Learning and Systems 2020},
 pages = {10776--10787},
 title = {Learning to Navigate in Synthetically Accessible Chemical Space Using Reinforcement Learning},
 year = {2020}
}

@ARTICLE{8936918,
author={T. {Ort} and K. {Murthy} and R. {Banerjee} and S. K. {Gottipati} and D. {Bhatt}
and I. {Gilitschenski} and L. {Paull} and D. {Rus}},
journal={IEEE Robotics and Automation Letters},
title={MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map},
year={2020},  volume={5},
number={2},  pages={556-563},}


@article{8784238,
author={S. K. {Gottipati} and K. {Seo} and D. {Bhatt} and V. {Mai} and K. {Murthy} and L. {Paull}},
journal={IEEE Robotics and Automation Letters},
title={Deep Active Localization},
year={2019},
volume={4},  number={4},
pages={4394-4401},
}

@INPROCEEDINGS{7989089,
  author={J. K. {Murthy} and G. V. S. {Krishna} and F. {Chhaya} and K. M. {Krishna}},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title={Reconstructing vehicles from a single image: Shape priors for road scene understanding},
  year={2017},
  volume={},
  number={},
  pages={724-731},}

@article{sai2018deep,
  title={Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting},
  author={Sai Krishna G, V and Goyette, Kyle and Chamseddine, Ahmad and Considine, Breandan},
  journal={preprint arXiv:1806.00683},
  year={2018}
}

@article{DBLP:journals/corr/abs-2010-03744,
  author       = {Sai Krishna Gottipati and
                  Yashaswi Pathak and
                  Rohan Nuttall and
                  Sahir and
                  Raviteja Chunduru and
                  Ahmed Touati and
                  Sriram Ganapathi Subramanian and
                  Matthew E. Taylor and
                  Sarath Chandar},
  title        = {Maximum Reward Formulation In Reinforcement Learning},
  journal      = {NeurIPS 2020 Deep RL Workshop},
  volume       = {abs/2010.03744},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.03744},
  eprinttype    = {arXiv},
  eprint       = {2010.03744},
  timestamp    = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-03744.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2106-11345,
  author       = {A. I. Redefined and
                  Sai Krishna Gottipati and
                  Sagar Kurandwad and
                  Clod{\'{e}}ric Mars and
                  Gregory Szriftgiser and
                  Fran{\c{c}}ois Chabot},
  title        = {Cogment: Open Source Framework For Distributed Multi-actor Training,
                  Deployment {\&} Operations},
  journal      = {AAMAS 2022 workshop on Multi-Agent Reinforcement Learning},
  volume       = {abs/2106.11345},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.11345},
  eprinttype    = {arXiv},
  eprint       = {2106.11345},
  timestamp    = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-11345.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Gottipati_Pathak_Sattarov_Sahir_Nuttall_Amini_Taylor_Chandar_2021, title={Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16087}, DOI={10.1609/aaai.v35i1.16087}, abstractNote={Reinforcement learning (RL) has made significant progress in both abstract and real-world domains, but the majority of state-of-the-art algorithms deal only with monotonic actions. However, some applications require agents to reason over different types of actions. Our application simulates reaction-based molecule generation, used as part of the drug discovery pipeline, and includes both uni-molecular and bi-molecular reactions. This paper introduces a novel framework, towered actor critic (TAC), to handle multiple action types. The TAC framework is general in that it is designed to be combined with any existing RL algorithms for continuous action space. We combine it with TD3 to empirically obtain significantly better results than existing methods in the drug discovery setting. TAC is also applied to RL benchmarks in OpenAI Gym and results show that our framework can improve, or at least does not hurt, performance relative to standard TD3.}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Gottipati, Sai Krishna and Pathak, Yashaswi and Sattarov, Boris and Sahir, and Nuttall, Rohan and Amini, Mohammad and Taylor, Matthew E. and Chandar, Sarath}, year={2021}, month={May}, pages={142-150} }

